\section{Approach}
	In this section we will present our aproach to tackle the speaker recognition problem.
\subsection{Erollment}
	\label{sec:approach_enrollment}
	An continuous speech of a user is collected during enrollment of that user. Depending on
	the number of users we plan to enroll and the accuracy of recognition system, duration
	of the utterance we need to collect may vary from 15 seconds to 30 seconds.

	Further processing of the utterance follows following steps:
	\begin{enumerate}
		\item Voice Activity Detection (VAD). \\
			Due to the channel difference and background noise may occur, which
			may degrade the performance of the system, non-speech part of the
			speech are removed from the utterance. This may lead to
			information loss (such as person's voice pause style), which in
			turns introduced the trade-off between loss of information and
			noise been modeled.

			As the corpus provided by teacher is nearly noise-free, we use a
			simple energy-based approach to remove the silence part.

		\item Extract MFCC feature representation. \\
			The utterance is then transformed to MFCC features with $20ms$
			frame-duration and $10ms$ frame-shift. Further more, we tuned the
			parameters of MFCC to yield better result. 40 filter banks and 19
			cepstral coefficent are parameters found to perform best.

		\item
          use Gaussian Mixture Model (or Continuous Restricted Boltzmann Machine) to model a user.

			As the MFCC feature of a utterance of a user is a high-dimensional point
			set, which comprised a distribution, we model a user by modeling its
			MFCC feature distribution. The generative model we have primally adopted is
			Gaussian Mixture Model(GMM). We have also conducted experiments on other generative model
			such as continuous version of Restricted Boltzmann Machine(RBM). Although it yields decent
			performance, but beated slightly by GMM.

		\item \textbf{JFA}:

          \textbf{Factor Analysis} is a typical method which behave
          very well in classification problems, due to its ability to
          account for different types of variability in training data.
          Within all the factor analysis methods,
          Joint Factor Analysis (JFA)\cite{jfa2,jfa-se} was proved to outperform other method
          in the task of Speaker Recognition.

          JFA models the user by ``supervector'' , i.e. a $C\times F $ dimension vector, where $C$ is
          the number of components in the Universal Background Model, trained by GMM on all the training data,
          and $ F$ is the dimension of the acoustic feature vector. The supervector of an utterance is obtained by concatenate
          all the $C $ means vectors in the trained GMM model. The basic assumption of JFA on describing a supervector is:

          \[ \vec{M} = \vec{ m } + vy + dz + ux, \]

          where $m$ is a supervector usually selected to be the one trained from UBM, $v$ is a $ CF \times R_s$ dimension matrix,
          $ u$ is a $ CF \times R_c$ dimension matrix, and $d$ is a diagonal matrix.
          This four variables are considered independent of all kinds of variabilities and remain constant after training, and
          $x, y, z $ are matrixes computed for each utterance sample.
          In this formulation, $ m + vy + dz$ is commonly believed to account for the ``Inter-Speaker Variability'', and $ux $ accounts
          for the ``Inter-Channel Variability''.
          The parameter $ R_s $ and $ R_c$, also referred to as ``Speaker Rank'' and ``Channel Rank'', are two emprical constant selected as first.
          The training of JFA is to calculate the best $ u, v, d$ to fit all the training data.

          However, the original algorithm \cite{jfa-se} for training JFA model is of
          too much complication and hard to implement.
          Therefore, we use the simpler algorithm presented in \cite{jfa-study}
          to train the JFA model.
	\end{enumerate}

\subsection{Recognition}
	Recognition procedure follows steps below:
	\begin{enumerate}
		\item Record a short utterance of the speaker (typically 5 seconds)
		\item Preprocess the utterance using first two steps described in
			\secref{approach_enrollment}, e.g, VAD and MFCC feature extraction.
		\item Compute the `score' of the MFCC featuresextraced for each model of persons
			enrolled, and adopt person corresponding the model which gives highest score to be the
			recognition result.

			A typical form of score is log-likelihood (or enery in RBM case).

	\end{enumerate}

